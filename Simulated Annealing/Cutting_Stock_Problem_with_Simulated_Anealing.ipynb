{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Main functions"
      ],
      "metadata": {
        "id": "zlZM_8Xt2mqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import math\n",
        "\n",
        "def calculate_fitness(solution, stock_length):\n",
        "    rolls = 1\n",
        "    remain = stock_length\n",
        "    for order in solution:\n",
        "        if order[\"length\"] <= remain:\n",
        "            remain -= order[\"length\"]\n",
        "        else:\n",
        "            rolls += 1\n",
        "            remain = stock_length - order[\"length\"]\n",
        "    return rolls\n",
        "\n",
        "def create_order(id, length):\n",
        "    return {\"id\": id, \"length\": length}\n",
        "\n",
        "\n",
        "def update_solution_for_neighborhood(next_solution, current_solution, best_ans, temperature):\n",
        "    next_cost = calculate_fitness(next_solution, stock_length)\n",
        "    delta = next_cost - calculate_fitness(current_solution, stock_length)\n",
        "    if delta < 0:\n",
        "        return next_solution, next_cost\n",
        "    else:\n",
        "        max_prob = math.exp((-delta) / temperature)\n",
        "        prob = random.random()\n",
        "        if prob < max_prob:\n",
        "            return next_solution, next_cost\n",
        "    return current_solution, best_ans\n",
        "\n",
        "def Simulated_Annealing(temperature, cooling_rate, stock_length, markov_length, orders, stop_criterion):\n",
        "    best_ans = math.inf\n",
        "    best_arrangement = []\n",
        "    solution = [create_order(i, length) for i, length in enumerate(orders)]\n",
        "    random.shuffle(solution)\n",
        "    outer_counter = 0\n",
        "    iteration = 0\n",
        "    while outer_counter < stop_criterion:\n",
        "        temperature = temperature * cooling_rate + 0.0001\n",
        "        pre_solution = solution[:]\n",
        "        n = 0\n",
        "        while n <= markov_length:\n",
        "            next_solution = solution[:]\n",
        "            point1 = random.randint(0, len(solution)-1)\n",
        "            point2 = random.randint(0, len(solution)-1)\n",
        "            next_solution[point1], next_solution[point2] = next_solution[point2], next_solution[point1]\n",
        "            solution, best_ans = update_solution_for_neighborhood(next_solution, solution, best_ans, temperature)\n",
        "            n += 1\n",
        "        if pre_solution == solution:\n",
        "            outer_counter += 1\n",
        "    return best_ans, solution\n"
      ],
      "metadata": {
        "id": "hiK1BR6bvdXd"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the Algorithm on input1.stock"
      ],
      "metadata": {
        "id": "3RvgGsze2rEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"input1.stock\", \"r\") as f:\n",
        "    txt = f.readlines()\n",
        "stock_length = int(txt[0][14:-1])\n",
        "req = list(map(int, txt[3].split(', ')))\n",
        "\n",
        "rolls, ans = Simulated_Annealing(temperature=1, cooling_rate=0.9, stock_length=stock_length, markov_length=3, orders=req, stop_criterion=500)\n",
        "print(\"Number of rolls used:\" , rolls)\n",
        "print(\"The final answer is: \")\n",
        "for i in ans:\n",
        "    print(i[\"length\"], end=' ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_MuOuwCCXYL",
        "outputId": "250cf037-cf22-454b-bdec-7f5ea3132fd2"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rolls used: 52\n",
            "The final answer is: \n",
            "515 463 914 123 868 144 805 441 118 45 125 266 312 106 532 333 592 75 495 457 306 351 301 544 409 753 107 149 753 92 518 119 354 632 232 230 525 125 69 933 618 292 648 286 686 280 662 356 557 987 278 171 84 405 424 315 218 86 437 312 88 43 116 501 211 71 60 70 295 80 988 402 581 414 371 115 109 224 627 92 148 653 249 557 88 149 53 460 337 248 716 170 145 672 788 180 412 555 286 507 549 368 557 246 106 46 967 264 678 187 126 268 370 23 788 181 241 61 18 135 506 689 106 284 517 186 79 266 106 365 346 33 251 149 78 609 283 99 117 660 "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the Algorithm on input2.stock"
      ],
      "metadata": {
        "id": "3prexrgw2ugL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"input2.stock\", \"r\") as f:\n",
        "    txt = f.readlines()\n",
        "stock_length = int(txt[0][14:-1])\n",
        "req = list(map(int, txt[3].split(', ')))\n",
        "\n",
        "rolls, ans = Simulated_Annealing(temperature=1, cooling_rate=0.9, stock_length=stock_length, markov_length=3, orders=req, stop_criterion=1100)\n",
        "print(\"Number of rolls used:\" , rolls)\n",
        "print(\"The final answer is: \")\n",
        "for i in ans:\n",
        "    print(i[\"length\"], end=' ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odEMaulwCXV2",
        "outputId": "0c629369-67f1-4678-efae-dc416bf44ccc"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rolls used: 79\n",
            "The final answer is: \n",
            "1880 2100 2100 1710 2140 2150 2000 1820 1560 1820 2000 1710 2140 1880 1560 2200 1880 2140 1380 1820 2150 1930 1520 2200 1380 1880 2140 1520 1820 1710 1560 2150 1820 1930 1820 1710 2150 1380 1820 2200 1520 2000 1520 1520 2140 1880 1380 1880 2140 1560 1520 2100 1710 2150 2050 1380 1380 1930 2100 1520 2050 1930 2050 1560 1880 1380 1820 2140 1520 1710 2050 1820 1930 1710 2200 2200 2050 2140 1380 1930 1520 2100 2000 1380 2150 2200 2100 2150 2200 1520 1880 2140 1880 2140 1520 2050 1520 1820 1560 1710 1930 1520 1820 2200 1380 2000 2100 2200 1710 1520 1820 1710 1930 2100 2200 1880 2150 1520 2140 2140 1820 2140 2150 1520 1930 1880 1520 1930 1380 2200 1820 1930 2100 1820 1380 2150 2100 2100 1930 1710 1880 2140 1560 1880 1930 1380 2200 2200 2000 1380 1380 1930 1710 2200 1520 1560 1520 2200 1380 1380 2140 1880 1560 2050 1880 2150 2200 2150 1820 2000 2100 1520 1930 2050 2000 2050 1380 1930 1560 2050 1710 1880 2000 2200 1820 1560 1930 1520 2150 1930 2150 1380 1520 2050 1880 2150 2150 1520 1880 2150 2200 1930 1380 1520 1820 2140 2200 1710 2150 1380 1520 2100 2100 2000 2200 1380 1560 1930 2050 "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the Algorithm on input3.stock"
      ],
      "metadata": {
        "id": "4Ffp7w8q2vx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"input3.stock\", \"r\") as f:\n",
        "    txt = f.readlines()\n",
        "stock_length = int(txt[0][14:-1])\n",
        "req = list(map(int, txt[3].split(', ')))\n",
        "\n",
        "rolls, ans = Simulated_Annealing(temperature=1, cooling_rate=0.9, stock_length=stock_length, markov_length=3, orders=req, stop_criterion=1000)\n",
        "print(\"Number of rolls used:\" , rolls)\n",
        "print(\"The final answer is: \")\n",
        "for i in ans:\n",
        "    print(i[\"length\"], end=' ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o15t7H-VwsJT",
        "outputId": "683a3bba-2e1c-42a9-a02e-7ff18034ffa9"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rolls used: 97\n",
            "The final answer is: \n",
            "1 3 8 6 28 209 2 4 188 16 6 7 205 18 7 196 8 13 138 3 18 13 318 5 3 314 150 288 1 9 10 153 5 3 7 5 11 4 3 34 10 2 411 15 25 4 16 3 21 433 1 7 8 2 271 8 11 9 174 8 2 7 1 5 9 14 421 40 11 61 4 38 1 3 11 14 19 163 7 4 1 12 144 11 2 24 1 6 13 6 12 16 10 3 403 3 144 311 6 14 110 2 5 1 1 360 7 12 7 8 314 124 18 5 2 153 1 27 4 3 3 1 22 4 1 265 7 5 7 315 66 2 4 198 4 6 4 1 10 8 7 7 243 2 16 2 13 1 3 2 3 4 2 32 1 350 7 49 1 225 1 152 9 1 50 152 1 186 3 152 199 2 1 5 234 3 21 9 11 3 361 1 4 12 21 6 12 9 7 8 15 7 7 11 4 3 170 4 126 161 8 3 1 3 5 8 6 4 3 1 6 9 11 271 6 91 2 2 9 4 76 17 228 11 1 25 3 26 7 154 6 7 386 6 6 1 2 78 2 1 10 5 2 5 183 9 3 3 275 1 5 14 23 343 39 159 92 237 1 4 11 7 8 24 4 9 3 2 138 16 12 3 8 227 172 112 2 213 92 3 263 204 264 3 6 13 281 8 11 1 8 4 16 162 430 4 15 3 3 1 224 2 4 7 86 43 9 120 1 2 5 8 35 1 11 2 7 314 111 2 10 1 399 14 9 6 9 13 2 9 10 16 6 13 191 2 6 229 76 1 4 419 1 5 9 6 13 34 125 52 14 7 18 12 4 10 3 2 3 6 11 160 3 3 364 359 49 147 5 7 2 158 8 66 1 1 106 128 240 7 5 405 45 1 21 23 98 9 6 3 1 4 6 5 2 3 4 167 147 8 3 49 7 3 313 11 260 189 17 1 9 87 5 1 38 2 8 197 17 8 134 6 180 23 4 234 5 6 1 275 29 21 2 139 2 1 225 1 255 6 118 9 11 15 1 1 1 18 298 124 290 458 14 7 245 20 19 7 6 170 3 1 7 1 7 92 299 2 2 93 72 5 369 135 14 13 3 134 178 24 5 4 244 7 12 2 27 6 152 1 7 250 68 2 7 5 4 4 4 8 1 87 5 264 41 5 1 152 13 9 313 10 88 4 1 3 12 16 12 41 13 224 239 7 6 2 3 6 87 90 319 116 8 5 339 3 3 7 12 12 9 1 115 335 17 18 74 2 5 1 1 18 9 321 2 4 479 8 5 18 5 2 170 5 9 109 3 1 1 4 14 6 102 9 470 3 9 2 6 34 147 2 1 21 21 14 201 6 8 6 15 11 5 5 9 5 4 167 2 16 79 17 166 12 16 5 1 151 3 2 174 7 124 8 8 133 6 187 3 2 5 156 9 4 12 11 2 1 1 5 9 97 132 1 2 14 15 282 2 86 1 19 17 26 1 4 4 131 2 111 243 8 11 11 9 7 6 1 24 6 15 3 27 2 85 8 277 14 134 6 59 11 6 99 7 17 134 165 23 108 98 5 21 9 9 36 138 2 6 4 4 68 7 1 3 5 2 3 8 2 2 3 4 2 159 4 4 10 4 3 314 19 169 24 5 2 245 4 2 9 1 1 8 3 54 3 1 7 6 9 263 5 13 10 43 9 13 1 6 2 3 2 19 280 4 116 10 1 2 14 10 184 2 1 6 5 2 1 1 3 3 214 10 3 7 1 4 6 7 2 3 96 6 108 279 5 12 9 133 10 14 1 5 17 5 1 7 209 67 5 15 3 167 1 9 245 26 169 331 2 2 5 255 3 102 8 9 3 5 2 9 2 14 2 10 17 4 18 8 9 7 1 5 1 92 130 243 7 8 9 10 1 6 5 178 251 37 204 2 86 6 2 157 2 2 10 8 2 4 1 5 12 274 6 8 14 75 89 1 7 "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the Algorithm on input4.stock"
      ],
      "metadata": {
        "id": "JHCklWRm2xZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"input4.stock\", \"r\") as f:\n",
        "    txt = f.readlines()\n",
        "stock_length = int(txt[0][14:-1])\n",
        "req = list(map(int, txt[3].split(', ')))\n",
        "\n",
        "rolls, ans = Simulated_Annealing(temperature=1, cooling_rate=0.9, stock_length=stock_length, markov_length=3, orders=req, stop_criterion=1000)\n",
        "print(\"Number of rolls used:\" , rolls)\n",
        "print(\"The final answer is: \")\n",
        "for i in ans:\n",
        "    print(i[\"length\"], end=' ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdYSGoEhCkvO",
        "outputId": "f285956c-1449-41b7-a4cf-a43ae09aa415"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rolls used: 221\n",
            "The final answer is: \n",
            "38 3 25 4 3 20 3 21 4 1 9 8 9 16 4 89 63 20 3 4 21 18 8 4 28 18 20 60 8 7 19 35 4 27 5 90 18 5 5 1 59 5 2 1 1 10 47 5 2 11 6 54 30 1 9 16 10 51 12 74 16 54 2 43 13 7 3 18 44 15 37 4 5 4 48 91 9 20 6 51 17 4 7 15 34 33 11 6 10 20 6 7 5 43 44 10 4 18 15 38 20 10 6 22 47 47 14 10 4 14 44 8 4 12 74 45 41 14 57 55 18 27 33 7 12 21 5 1 27 26 6 31 6 3 47 37 3 3 9 26 51 79 16 30 30 16 33 62 71 6 5 5 13 15 23 4 14 33 6 1 2 32 5 19 33 10 4 59 28 13 12 1 74 12 14 44 4 48 8 13 10 37 4 34 2 5 60 22 5 1 9 4 6 36 46 2 82 56 10 33 80 3 15 8 37 5 5 38 8 5 37 9 15 59 5 11 22 17 40 5 7 9 5 12 21 8 2 9 36 21 8 92 12 6 4 63 66 33 7 27 36 52 35 8 40 35 23 73 54 25 18 45 10 3 8 16 1 12 52 16 58 14 12 10 13 81 66 74 7 24 7 14 3 6 17 18 10 25 62 6 15 6 16 46 10 33 6 2 48 92 71 28 21 6 2 6 14 47 8 47 8 37 16 12 5 89 6 2 7 5 34 21 9 15 30 15 5 5 7 37 8 3 24 2 46 72 19 3 6 13 33 20 52 15 32 24 51 21 65 3 28 15 2 5 9 4 12 51 16 47 1 1 22 6 15 12 67 32 5 51 31 5 3 34 26 88 11 3 16 32 19 19 18 8 7 13 45 5 4 2 34 50 13 32 18 8 18 47 9 8 9 26 21 4 13 1 52 32 67 2 24 25 1 28 18 15 4 8 39 25 12 5 3 27 6 6 7 46 38 7 4 80 44 56 67 16 12 7 4 67 6 31 67 3 3 73 13 2 43 6 14 16 11 18 23 13 19 24 21 2 11 45 20 19 1 9 10 56 27 20 52 29 22 23 10 1 42 55 6 12 80 56 17 6 66 34 11 58 3 6 12 6 5 61 31 7 14 6 36 31 27 27 13 23 12 26 6 2 12 26 9 78 8 1 9 34 6 20 36 10 75 5 5 2 78 68 69 10 2 12 24 15 18 17 3 22 18 9 2 11 21 10 25 6 14 13 35 31 2 95 2 37 26 4 28 93 8 7 2 3 38 4 36 36 2 26 18 2 36 44 13 68 10 14 8 11 5 16 7 59 25 16 50 14 2 24 22 34 46 31 8 5 74 1 24 5 17 73 98 22 69 98 47 1 7 18 5 4 81 16 38 4 18 39 24 30 44 28 21 48 53 35 1 1 7 72 14 7 23 30 6 39 37 2 3 3 3 17 6 20 1 54 10 11 7 18 20 22 38 3 17 23 3 4 65 99 10 6 9 2 11 61 19 30 9 5 9 15 11 2 16 54 18 7 3 79 24 17 46 4 7 16 20 11 2 46 6 87 5 25 1 4 33 7 5 22 15 34 24 21 4 10 29 4 2 3 16 3 13 20 15 7 76 31 14 43 3 21 66 39 61 2 29 16 47 5 2 13 1 2 7 5 3 4 62 25 30 19 3 4 3 50 5 44 72 3 20 17 50 27 93 3 20 47 30 9 89 5 56 19 21 6 4 47 21 6 2 15 18 58 1 3 38 1 34 14 9 33 7 50 22 8 65 12 24 3 26 20 8 13 6 80 71 24 58 1 19 22 70 7 20 26 5 1 4 23 26 9 34 57 5 87 59 2 32 84 2 5 68 13 13 1 7 8 9 4 27 27 15 4 22 73 51 31 2 11 20 41 2 8 26 2 46 2 38 91 18 4 32 22 20 32 7 23 9 6 65 4 29 3 61 1 3 22 2 29 7 39 24 9 11 21 5 54 35 15 10 4 17 13 14 51 2 32 43 1 14 35 6 1 43 56 4 23 20 29 5 56 2 13 14 20 68 3 1 6 1 43 40 14 "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional part: solving traveling salesman problem wih simulated annealing"
      ],
      "metadata": {
        "id": "-Nt1ViAoQl94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "\n",
        "def calculate_total_distance(distance_matrix, tour):\n",
        "    \"\"\"\n",
        "    Calculate the total distance of a given tour using a distance matrix.\n",
        "    \"\"\"\n",
        "    num_cities = len(tour)\n",
        "    total_distance = 0\n",
        "    for i in range(num_cities):\n",
        "        total_distance += distance_matrix[tour[i], tour[(i + 1) % num_cities]]\n",
        "    return total_distance\n",
        "\n",
        "def get_neighbor(tour):\n",
        "    \"\"\"\n",
        "    Generate a neighboring tour by swapping two cities.\n",
        "    \"\"\"\n",
        "    num_cities = len(tour)\n",
        "    i = random.randint(0, num_cities - 1)\n",
        "    j = random.randint(0, num_cities - 1)\n",
        "    new_tour = tour.copy()\n",
        "    new_tour[i], new_tour[j] = new_tour[j], new_tour[i]\n",
        "    return new_tour\n",
        "\n",
        "def acceptance_probability(old_cost, new_cost, temperature):\n",
        "    \"\"\"\n",
        "    Calculate the acceptance probability for a new tour.\n",
        "    \"\"\"\n",
        "    if new_cost < old_cost:\n",
        "        return 1.0\n",
        "    return math.exp((old_cost - new_cost) / temperature)\n",
        "\n",
        "def simulated_annealing(distance_matrix, initial_temperature, cooling_rate, num_iterations, markov_length):\n",
        "    \"\"\"\n",
        "    Solve the TSP using simulated annealing with a distance matrix and Markov length.\n",
        "    \"\"\"\n",
        "    num_cities = distance_matrix.shape[0]\n",
        "    tour = random.sample(range(num_cities), num_cities)\n",
        "    best_tour = tour.copy()\n",
        "    current_cost = calculate_total_distance(distance_matrix, tour)\n",
        "    best_cost = current_cost\n",
        "    temperature = initial_temperature\n",
        "    iterations_since_best = 0\n",
        "\n",
        "    for iteration in range(num_iterations):\n",
        "        new_tour = get_neighbor(tour)\n",
        "        new_cost = calculate_total_distance(distance_matrix, new_tour)\n",
        "        if acceptance_probability(current_cost, new_cost, temperature) > random.random():\n",
        "            tour = new_tour\n",
        "            current_cost = new_cost\n",
        "        if current_cost < best_cost:\n",
        "            best_tour = tour.copy()\n",
        "            best_cost = current_cost\n",
        "            iterations_since_best = 0\n",
        "        else:\n",
        "            iterations_since_best += 1\n",
        "        if iterations_since_best >= markov_length:\n",
        "            temperature *= cooling_rate\n",
        "            iterations_since_best = 0\n",
        "\n",
        "    return best_tour, best_cost\n",
        "\n",
        "f = open('gr229.tsp')\n",
        "inp = f.read()\n",
        "f.close()\n",
        "nodes = [0] * len(inp.split('\\n')[7:-2])\n",
        "t = 0\n",
        "for e in inp.split('\\n')[7:-2]:\n",
        "    nodes[t] = (float(e.split()[1]),float(e.split()[2]))\n",
        "    t += 1\n",
        "matrix = [[0 for i in range(len(nodes))] for j in range(len(nodes))]\n",
        "for i in range(len(nodes)):\n",
        "    for j in range(len(nodes)):\n",
        "        matrix[i][j] = sqrt((nodes[i][0]-nodes[j][0])**2+(nodes[i][1]-nodes[j][1])**2)"
      ],
      "metadata": {
        "id": "0fjHdB3Vjlc-"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the Algorithm on gr229.rsp"
      ],
      "metadata": {
        "id": "eQtyu2-O27LP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distance_matrix = np.array(matrix)\n",
        "\n",
        "initial_temperature = 100000.0\n",
        "cooling_rate = 0.99\n",
        "num_iterations = 200000\n",
        "markov_length = 100\n",
        "\n",
        "best_tour, best_cost = simulated_annealing(distance_matrix, initial_temperature, cooling_rate, num_iterations, markov_length)\n",
        "\n",
        "print(\"Best tour:\", best_tour)\n",
        "print(\"Best cost:\", best_cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrVeJ957x1Jm",
        "outputId": "80939571-5148-4701-a8af-0d4bee780513"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best tour: [178, 173, 172, 147, 126, 102, 92, 93, 94, 98, 111, 303, 283, 462, 370, 366, 385, 314, 323, 313, 315, 320, 319, 44, 41, 312, 317, 382, 381, 367, 373, 636, 435, 374, 378, 379, 396, 324, 316, 272, 227, 223, 235, 236, 219, 498, 436, 729, 702, 703, 763, 762, 772, 759, 761, 892, 863, 858, 565, 516, 523, 524, 520, 517, 814, 860, 859, 1001, 830, 831, 832, 857, 856, 852, 839, 181, 180, 196, 195, 510, 810, 916, 918, 882, 922, 924, 908, 792, 604, 610, 363, 352, 347, 718, 951, 946, 945, 942, 953, 954, 955, 992, 752, 493, 189, 237, 233, 234, 220, 494, 825, 861, 864, 874, 871, 862, 854, 817, 449, 433, 338, 335, 334, 388, 389, 652, 448, 559, 561, 544, 538, 818, 813, 811, 805, 756, 999, 764, 967, 980, 944, 984, 987, 981, 674, 677, 676, 675, 679, 686, 713, 349, 391, 410, 996, 458, 455, 424, 414, 405, 409, 291, 22, 8, 7, 6, 68, 55, 57, 46, 380, 387, 350, 392, 289, 295, 288, 290, 397, 359, 351, 661, 725, 682, 988, 985, 696, 697, 689, 671, 672, 688, 685, 692, 715, 716, 687, 680, 717, 720, 668, 664, 66, 69, 79, 77, 95, 131, 142, 148, 267, 491, 568, 549, 550, 551, 574, 575, 59, 58, 64, 56, 54, 80, 82, 81, 75, 0, 1, 70, 72, 468, 271, 143, 145, 155, 186, 188, 262, 268, 997, 823, 900, 904, 907, 821, 554, 555, 594, 632, 634, 645, 653, 657, 362, 354, 659, 670, 673, 666, 662, 353, 357, 356, 660, 654, 641, 646, 639, 650, 730, 734, 721, 723, 726, 622, 479, 260, 250, 240, 187, 193, 207, 203, 202, 200, 514, 553, 975, 973, 965, 782, 781, 915, 877, 844, 895, 919, 774, 778, 783, 793, 584, 122, 123, 104, 34, 23, 61, 62, 63, 35, 294, 426, 425, 118, 117, 993, 101, 253, 251, 231, 482, 513, 566, 509, 500, 499, 488, 490, 806, 799, 933, 934, 914, 905, 906, 828, 540, 542, 757, 767, 735, 616, 649, 638, 375, 415, 282, 280, 125, 164, 157, 158, 184, 163, 165, 141, 139, 138, 132, 112, 635, 613, 788, 787, 797, 590, 589, 573, 577, 576, 582, 611, 669, 665, 656, 360, 361, 369, 416, 429, 430, 631, 628, 749, 743, 714, 694, 693, 691, 971, 512, 505, 502, 501, 507, 528, 564, 572, 579, 298, 42, 45, 318, 402, 427, 442, 440, 438, 437, 454, 457, 489, 525, 541, 929, 775, 771, 958, 957, 959, 807, 539, 536, 537, 521, 515, 519, 563, 569, 459, 40, 36, 60, 65, 71, 67, 13, 15, 38, 310, 647, 619, 931, 930, 760, 732, 748, 731, 614, 626, 609, 432, 431, 194, 179, 177, 156, 244, 273, 460, 620, 655, 658, 358, 339, 394, 461, 428, 229, 239, 241, 242, 243, 259, 119, 297, 417, 372, 637, 711, 683, 684, 681, 678, 977, 962, 961, 943, 947, 707, 706, 277, 300, 301, 108, 53, 51, 134, 137, 130, 140, 168, 174, 169, 403, 406, 404, 423, 533, 535, 511, 481, 475, 474, 640, 642, 364, 395, 326, 328, 28, 14, 9, 5, 73, 87, 106, 269, 230, 266, 477, 578, 824, 804, 1000, 938, 939, 952, 701, 708, 695, 986, 982, 979, 948, 950, 776, 917, 920, 886, 881, 925, 932, 937, 940, 742, 744, 911, 777, 784, 766, 970, 983, 989, 709, 618, 623, 453, 210, 211, 213, 212, 215, 496, 495, 497, 216, 206, 204, 153, 152, 258, 439, 633, 612, 602, 596, 593, 560, 562, 557, 803, 935, 936, 960, 949, 976, 978, 974, 690, 724, 728, 371, 377, 383, 321, 292, 286, 287, 407, 727, 722, 710, 712, 456, 472, 224, 222, 232, 218, 217, 208, 503, 508, 829, 834, 815, 812, 921, 926, 927, 928, 869, 868, 865, 879, 880, 923, 899, 898, 885, 878, 890, 888, 846, 842, 837, 833, 816, 827, 526, 506, 994, 252, 248, 263, 580, 733, 745, 741, 615, 607, 794, 910, 909, 913, 912, 595, 586, 585, 588, 545, 531, 530, 522, 274, 275, 296, 293, 322, 325, 25, 21, 4, 2, 3, 10, 20, 332, 412, 447, 546, 835, 836, 893, 894, 891, 889, 875, 843, 476, 471, 469, 279, 281, 990, 340, 343, 342, 341, 336, 18, 27, 26, 29, 327, 399, 386, 355, 663, 719, 780, 798, 822, 552, 581, 583, 450, 306, 85, 74, 78, 90, 97, 128, 146, 150, 149, 247, 261, 245, 264, 548, 826, 840, 849, 866, 867, 873, 872, 870, 802, 629, 441, 630, 627, 644, 651, 643, 648, 991, 625, 411, 413, 284, 422, 420, 470, 820, 897, 847, 848, 853, 855, 850, 838, 543, 547, 558, 483, 492, 228, 265, 226, 225, 110, 11, 12, 16, 30, 330, 329, 331, 337, 700, 972, 699, 740, 753, 800, 801, 598, 209, 192, 246, 120, 276, 398, 400, 384, 421, 418, 376, 368, 408, 401, 307, 304, 114, 115, 127, 124, 154, 183, 182, 205, 197, 485, 484, 478, 480, 473, 348, 346, 345, 390, 452, 451, 214, 238, 191, 255, 278, 467, 444, 443, 446, 445, 434, 419, 285, 109, 99, 133, 96, 89, 86, 76, 52, 50, 49, 43, 305, 221, 199, 198, 185, 257, 105, 116, 302, 309, 311, 83, 88, 91, 136, 256, 621, 617, 737, 704, 705, 606, 995, 160, 161, 162, 170, 190, 487, 486, 998, 529, 518, 567, 570, 592, 556, 819, 809, 808, 903, 902, 901, 876, 845, 851, 534, 532, 571, 591, 587, 790, 789, 750, 751, 747, 773, 956, 941, 769, 605, 603, 597, 599, 786, 698, 969, 964, 963, 884, 883, 887, 896, 841, 527, 504, 201, 171, 159, 167, 151, 39, 31, 24, 344, 667, 968, 966, 765, 768, 770, 779, 785, 755, 758, 365, 333, 32, 33, 37, 84, 103, 254, 249, 270, 466, 608, 624, 796, 795, 600, 601, 791, 754, 746, 739, 736, 738, 393, 19, 17, 47, 107, 113, 121, 464, 465, 463, 299, 308, 48, 100, 129, 135, 144, 166, 176, 175]\n",
            "Best cost: 1377571.7827984672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the Algorithm on pr1002.tsp"
      ],
      "metadata": {
        "id": "KFpeSnsm297b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('pr1002.tsp')\n",
        "inp = f.read()\n",
        "f.close()\n",
        "nodes = [0] * len(inp.split('\\n')[6:-1])\n",
        "t = 0\n",
        "for e in inp.split('\\n')[6:-1]:\n",
        "    nodes[t] = (float(e.split()[1]),float(e.split()[2]))\n",
        "    t += 1\n",
        "matrix = [[0 for i in range(len(nodes))] for j in range(len(nodes))]\n",
        "for i in range(len(nodes)):\n",
        "    for j in range(len(nodes)):\n",
        "        matrix[i][j] = sqrt((nodes[i][0]-nodes[j][0])**2+(nodes[i][1]-nodes[j][1])**2)\n",
        "\n",
        "\n",
        "\n",
        "distance_matrix = np.array(matrix)\n",
        "\n",
        "initial_temperature = 100000.0\n",
        "cooling_rate = 0.99\n",
        "num_iterations = 200000\n",
        "markov_length = 100\n",
        "\n",
        "best_tour, best_cost = simulated_annealing(distance_matrix, initial_temperature, cooling_rate, num_iterations, markov_length)\n",
        "\n",
        "print(\"Best tour:\", best_tour)\n",
        "print(\"Best cost:\", best_cost)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nJRDE4Vokbx",
        "outputId": "6a946adf-4685-4f91-be42-313f004b84cb"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best tour: [653, 369, 376, 396, 36, 51, 81, 297, 276, 280, 116, 148, 149, 150, 141, 142, 432, 452, 454, 448, 594, 555, 593, 554, 798, 804, 800, 759, 760, 770, 758, 910, 901, 876, 875, 872, 891, 893, 896, 897, 767, 691, 687, 950, 951, 955, 937, 931, 905, 809, 807, 557, 234, 233, 221, 222, 220, 232, 231, 444, 443, 450, 631, 793, 911, 913, 907, 811, 820, 563, 484, 483, 602, 708, 711, 714, 697, 700, 962, 824, 542, 526, 539, 517, 518, 508, 212, 244, 128, 127, 132, 95, 82, 107, 123, 238, 210, 211, 487, 529, 525, 544, 564, 548, 546, 543, 828, 830, 831, 1001, 859, 861, 865, 853, 849, 874, 889, 878, 885, 883, 919, 895, 890, 892, 841, 906, 808, 799, 803, 762, 765, 764, 773, 959, 939, 938, 933, 929, 930, 916, 923, 926, 915, 774, 756, 752, 744, 741, 947, 976, 942, 940, 783, 784, 954, 952, 945, 984, 973, 975, 978, 972, 966, 792, 556, 560, 1000, 932, 944, 943, 763, 794, 588, 574, 471, 479, 488, 522, 514, 629, 628, 613, 748, 702, 766, 779, 777, 802, 577, 580, 301, 113, 423, 421, 427, 428, 550, 558, 559, 553, 552, 823, 850, 845, 844, 871, 869, 870, 868, 873, 848, 894, 815, 816, 538, 545, 567, 561, 551, 818, 887, 886, 840, 817, 810, 791, 709, 667, 671, 755, 822, 521, 510, 511, 497, 216, 218, 256, 100, 133, 103, 281, 284, 433, 600, 805, 852, 854, 863, 860, 862, 866, 877, 980, 979, 988, 989, 967, 698, 712, 743, 742, 819, 826, 534, 531, 197, 186, 194, 240, 314, 38, 66, 68, 65, 67, 71, 63, 37, 401, 405, 403, 286, 285, 463, 424, 419, 425, 434, 614, 750, 801, 888, 846, 864, 537, 536, 535, 504, 503, 263, 228, 245, 279, 291, 289, 295, 317, 111, 135, 138, 145, 147, 144, 131, 168, 175, 174, 169, 115, 300, 303, 299, 449, 573, 570, 568, 250, 137, 129, 136, 306, 311, 318, 35, 29, 23, 28, 332, 388, 359, 723, 688, 689, 963, 701, 659, 354, 670, 679, 666, 664, 663, 591, 566, 524, 998, 506, 500, 509, 507, 505, 946, 948, 949, 489, 490, 204, 208, 496, 491, 575, 827, 851, 858, 530, 501, 206, 205, 198, 187, 242, 224, 533, 532, 527, 215, 161, 163, 164, 134, 77, 69, 73, 139, 166, 162, 157, 158, 182, 219, 213, 520, 519, 515, 512, 252, 92, 93, 90, 414, 626, 961, 941, 936, 771, 776, 908, 842, 838, 839, 835, 836, 541, 549, 562, 470, 472, 226, 193, 180, 178, 172, 179, 183, 188, 189, 192, 190, 266, 260, 365, 361, 360, 437, 440, 101, 84, 85, 87, 88, 91, 97, 76, 78, 57, 54, 49, 34, 70, 74, 94, 99, 98, 79, 80, 47, 294, 378, 366, 374, 375, 380, 43, 45, 316, 309, 464, 457, 459, 422, 185, 195, 200, 202, 199, 207, 209, 502, 898, 900, 903, 904, 909, 780, 757, 754, 738, 736, 649, 646, 638, 625, 611, 607, 605, 814, 812, 847, 843, 821, 603, 632, 371, 379, 410, 268, 229, 225, 590, 589, 592, 296, 322, 326, 327, 387, 641, 652, 724, 719, 690, 960, 992, 953, 987, 681, 673, 676, 677, 633, 435, 373, 634, 367, 337, 339, 386, 460, 465, 466, 269, 262, 264, 237, 239, 482, 813, 899, 918, 879, 880, 882, 782, 344, 343, 342, 14, 13, 6, 17, 581, 585, 786, 789, 790, 731, 648, 436, 426, 418, 411, 398, 395, 293, 315, 319, 323, 324, 329, 321, 278, 236, 498, 485, 477, 621, 725, 722, 707, 616, 624, 441, 458, 473, 469, 494, 227, 243, 152, 153, 994, 990, 382, 393, 389, 348, 686, 683, 985, 983, 977, 971, 974, 964, 965, 968, 692, 678, 662, 347, 384, 381, 331, 25, 52, 42, 406, 768, 934, 956, 957, 914, 806, 599, 583, 445, 446, 442, 630, 451, 447, 825, 837, 857, 856, 855, 785, 787, 412, 72, 4, 2, 5, 1, 10, 9, 20, 31, 22, 21, 16, 15, 19, 18, 24, 288, 407, 377, 656, 665, 668, 672, 769, 928, 927, 912, 726, 353, 346, 340, 336, 335, 325, 313, 39, 109, 456, 474, 475, 155, 165, 154, 251, 247, 246, 249, 184, 160, 223, 495, 492, 997, 523, 528, 516, 383, 394, 368, 642, 372, 643, 651, 654, 623, 615, 735, 704, 693, 695, 703, 706, 696, 699, 970, 969, 986, 981, 705, 713, 734, 746, 747, 796, 598, 587, 596, 797, 999, 740, 737, 661, 357, 358, 352, 391, 338, 330, 310, 41, 298, 125, 126, 143, 102, 118, 117, 993, 86, 56, 50, 48, 46, 114, 105, 255, 258, 257, 273, 275, 461, 409, 408, 416, 404, 402, 292, 44, 307, 304, 110, 302, 305, 290, 397, 59, 58, 55, 283, 277, 480, 569, 565, 547, 540, 601, 612, 622, 647, 399, 400, 320, 308, 265, 230, 261, 259, 270, 267, 467, 627, 716, 720, 715, 618, 620, 644, 658, 287, 282, 312, 27, 26, 33, 333, 370, 639, 749, 753, 733, 430, 120, 130, 146, 140, 124, 108, 40, 32, 30, 61, 62, 151, 156, 176, 177, 995, 203, 201, 196, 241, 274, 727, 717, 680, 682, 982, 684, 685, 728, 608, 609, 438, 417, 420, 462, 455, 586, 597, 576, 595, 917, 881, 867, 829, 834, 584, 453, 429, 991, 675, 674, 669, 637, 415, 119, 121, 122, 96, 53, 328, 356, 655, 729, 751, 788, 781, 920, 922, 924, 925, 935, 958, 775, 778, 772, 761, 481, 181, 173, 159, 170, 171, 167, 75, 8, 7, 11, 12, 390, 350, 351, 362, 640, 636, 635, 606, 572, 571, 579, 578, 513, 486, 499, 493, 271, 112, 104, 106, 60, 64, 0, 3, 89, 83, 334, 341, 349, 345, 392, 385, 413, 214, 217, 191, 235, 478, 476, 996, 645, 660, 355, 364, 363, 657, 617, 902, 921, 884, 833, 832, 795, 739, 710, 694, 718, 721, 730, 610, 439, 431, 272, 254, 253, 248, 468, 582, 604, 745, 732, 619, 650]\n",
            "Best cost: 1410948.5436529103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7s-DAK5Mxw9d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}